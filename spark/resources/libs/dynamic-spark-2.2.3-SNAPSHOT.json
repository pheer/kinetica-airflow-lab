{
  "properties": {
    "doc.ScalaSparkProgram-sparkprogram": "# Spark Program in Scala\n\nDescription\n-----------\nExecutes user-provided Spark code in Scala.\n\nUse Case\n--------\nThis plugin can be used when you want arbitrary Spark code.\n\nProperties\n----------\n**mainClass** The fully qualified class name for the Spark application.\nIt must either be an ``object`` that has a ``main`` method define inside, with the method signature as \n``def main(args: Array[String]): Unit``; or it is a class that extends from the CDAP \n``io.cdap.cdap.api.spark.SparkMain`` trait that implements the ``run`` method, with the method signature as\n``def run(implicit sec: SparkExecutionContext): Unit``\n\n**scalaCode** The self-contained Spark application written in Scala.\nFor example, an application that reads from CDAP stream with name ``streamName``, \nperforms a simple word count logic and logs the result can be written as:\n\n    import io.cdap.cdap.api.spark._\n    import org.apache.spark._\n    import org.slf4j._\n\n    class SparkProgram extends SparkMain {\n      import SparkProgram._\n\n      override def run(implicit sec: SparkExecutionContext): Unit = {\n        val sc = new SparkContext\n        val result = sc.fromStream[String](\"streamName\")\n          .flatMap(_.split(\"\\\\s+\"))\n          .map((_, 1))\n          .reduceByKey(_ + _)\n          .collectAsMap\n          \n        LOG.info(\"Result is: {}\", result)\n      }\n    }\n\n    object SparkProgram {\n      val LOG = LoggerFactory.getLogger(getClass())\n    }\n \n**dependencies** Extra dependencies for the Spark program.\nIt is a ',' separated list of URI for the location of dependency jars.\nA path can be ended with an asterisk '*' as a wildcard, in which all files with extension '.jar' under the\nparent path will be included.\n\n**deployCompile** Specify whether the code will get validated during pipeline creation time. Setting this to `false`\nwill skip the validation.\n \nPlease refer to the [CDAP documentation](https://docs.cask.co/cdap/current/en/developers-manual/building-blocks/spark-programs.html#cdap-spark-program) on the enhancements that CDAP brings to Spark.",
    "doc.PySparkProgram-sparkprogram": "# Spark Program in Python\n\nDescription\n-----------\nExecutes user-provided Spark code in Python.\n\nUse Case\n--------\nThis plugin can be used when you want to run arbitrary Spark code.\n\nProperties\n----------\n* **pythonCode**\n \n  The self-contained Spark application written in Python.\n  For example, the [Naive Bayes Machine Learning](https://spark.apache.org/docs/1.6.3/mllib-naive-bayes.html) \n  from the official Spark documentation can be written as:\n\n      # Import libraries\n      from pyspark import *\n      from pyspark.mllib.classification import NaiveBayes, NaiveBayesModel\n      from pyspark.mllib.linalg import Vectors\n      from pyspark.mllib.regression import LabeledPoint\n      \n      def parseLine(line):\n          parts = line.split(',')\n          label = float(parts[0])\n          features = Vectors.dense([float(x) for x in parts[1].split(' ')])\n          return LabeledPoint(label, features)\n      \n      sc = SparkContext()\n      data = sc.textFile(\"${input.path}\").map(parseLine)\n      \n      # Split data aproximately into training (60%) and test (40%)\n      training, test = data.randomSplit([0.6, 0.4], seed=0)\n      \n      # Train a naive Bayes model.\n      model = NaiveBayes.train(training, 1.0)\n      \n      # Make prediction and test accuracy.\n      predictionAndLabel = test.map(lambda p: (model.predict(p.features), p.label))\n      accuracy = 1.0 * predictionAndLabel.filter(lambda (x, v): x == v).count() / test.count()\n      \n      # Save and load model\n      model.save(sc, \"${output.path}\")\n\n  With the `input.path` and `output.path` being provided at runtime. \n \n* **pyFiles** \n\n  Extra libraries for the PySpark program. \n  It is a ',' separated list of URI for the locations of extra .egg, .zip and .py libraries.\n",
    "widgets.ScalaSparkCompute-sparkcompute": "{\n  \"outputs\": [{\n    \"name\": \"schema\",\n    \"widget-type\": \"schema\",\n    \"widget-attributes\": {\n      \"schema-types\": [\n        \"boolean\",\n        \"int\",\n        \"long\",\n        \"float\",\n        \"double\",\n        \"bytes\",\n        \"string\",\n        \"map<string, string>\"\n      ],\n      \"schema-default-type\": \"string\"\n    }\n  }],\n  \"metadata\": {\"spec-version\": \"1.4\"},\n  \"configuration-groups\": [{\n    \"label\": \"Spark Scala\",\n    \"properties\": [\n      {\n        \"widget-type\": \"scala-editor\",\n        \"name\": \"scalaCode\",\n        \"label\": \"Scala\",\n        \"widget-attributes\": {\"default\": \"/**\\n * Transforms the provided input Apache Spark RDD or DataFrame into another RDD or DataFrame.\\n *\\n * The input DataFrame has the same schema as the input schema to this stage and the transform method should return a DataFrame that has the same schema as the output schema setup for this stage.\\n * To emit logs, use: \\n *     import org.slf4j.LoggerFactory\\n *     val logger = LoggerFactory.getLogger('mylogger')\\n *     logger.info('Logging')\\n *\\n *\\n * @param input the input DataFrame which has the same schema as the input schema to this stage.\\n * @param context a SparkExecutionPluginContext object that can be used to emit zero or more records (using the emitter.emit() method) or errors (using the emitter.emitError() method) \\n * @param context an object that provides access to:\\n *      1. CDAP Datasets and Streams - context.fromDataset('counts'); or context.fromStream('input');\\n *      2. Original Spark Context - context.getSparkContext();\\n *      3. Runtime Arguments - context.getArguments.get('priceThreshold')\\n */\\ndef transform(df: DataFrame, context: SparkExecutionPluginContext) : DataFrame = {\\n  df\\n}\"}\n      },\n      {\n        \"widget-type\": \"csv\",\n        \"name\": \"dependencies\",\n        \"label\": \"Dependencies\"\n      },\n      {\n        \"widget-type\": \"select\",\n        \"name\": \"deployCompile\",\n        \"label\": \"Compile at Deployment Time\",\n        \"widget-attributes\": {\n          \"default\": \"true\",\n          \"values\": [\n            \"true\",\n            \"false\"\n          ]\n        }\n      }\n    ]\n  }],\n  \"display-name\": \"Spark\",\n  \"icon\": {\n    \"arguments\": {\"data\": \"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAADY0lEQVRYR8XXd+i2YxQH8M+bTcab\\r\\nkZlVFJKtkJU/KHv1mplFyYgosikjWZFSRiIlkr0yMjJCsopCmSkrK1vfX9f963rv7uf3XM/zB+ev\\r\\n577vc53zvc74nvPM8z/LvCn8L4ubsT++wYm4fwo7M0emAXA7jqwc/oEN8fE0ICYFsAh+wDI9Z3fh\\r\\nsP8CwGZ4c4SjLeb4NhLbpBE4HHcUa99jKSxRnt/Fpvh7TCQ2wir4EJ9PCuAynFUcPIEXcWHl8Fjc\\r\\n0gOwBrZDwG+PFcv3z7DWpAAewF7FwFU4B+9hvfLuq+JkH+yKbbHyiIgkgvMnBfAR1i0Gj0I6Yhc8\\r\\nPUUBXoPTJgGwOH6rHG2JN8rzPThwBIif8RJ2q9o+79YOj7QCWAxn44LiJIWWAvy9PK+KRCfvIl/g\\r\\nQSRlTxXiqrnjFFw3johy4+Q7od4Ry1U3/KCQT33pY8qt4vT16sOhuLN6vg8HdM/9CCyKbQrTLcDy\\r\\nI8J67xwhr4+kXt7B0lVkNkBSMCMdgCVxRbltuH5IvsYjeAiP46cxhZfLvIrNq7TthBfqcx2AK3HG\\r\\ngMEU2cPF6Wv4Z4JqvxxnVvoX47z++Q7Ap1iz9/EmnI5fJnDaqabiQ1Sd/ZeR23dFO2uyU3i2KPR9\\r\\nJeyXIGAy9VolRZfi6+RuHDJ0uAOQnsxEC2UOySc4v8yBljRshaSsk7Tt+oidhaTfBSGT1MM6Ayhu\\r\\nQPr3r8YwPIOdK93bcPQ4APke0jmh3DiDIw6Px63lcEBnL/hzDJDd8WilE/1EOiQ1K3MxYdoxVZu2\\r\\ne646c2kBNwRgv6L7bdF/G5tUZ88tNdUEYOiCByGpyDzvJDScKRh5Cxvj+bInhl8ywjtJtyW9sztD\\r\\n6yyIga2L4fcrclm9EFhmfeTJMnTmys4eeKxTaAWQnH9ZZnuIaU+sgPR3mO24YrDffkNATsW1kwLI\\r\\nVpMNJpKV/OSS68yNbEih8UiW1SNwcNkThgBkUUmHzEhrBOLolXLmojKwUuWRvcvo7TsLF+yLbEcp\\r\\nxBBZSC3nZ6UVQP6EZAJGkorVyu9UeSj2u6GrVu8SwQyvrPQLSSuAk3B972xaM203zvmc2FoBpH/r\\r\\n0GXpSFRaWXEkiFYA+f93Y7GSEb0Dfh0T9qbPrQBWwtWFQJKOH5usNyi1AmgwNZ3Kv9VaoSGlep5g\\r\\nAAAAAElFTkSuQmCC\"},\n    \"type\": \"inline\"\n  }\n}",
    "widgets.ScalaSparkSink-sparksink": "{\n  \"metadata\": {\"spec-version\": \"1.4\"},\n  \"configuration-groups\": [{\n    \"label\": \"Spark Scala\",\n    \"properties\": [\n      {\n        \"widget-type\": \"scala-editor\",\n        \"name\": \"scalaCode\",\n        \"label\": \"Scala\",\n        \"widget-attributes\": {\"default\": \"/**\\n * Performs operations on the provided input Apache Spark RDD or DataFrame.\\n *\\n * The input DataFrame has the same schema as the input schema to this stage.\\n * To emit logs, use: \\n *     import org.slf4j.LoggerFactory\\n *     val logger = LoggerFactory.getLogger('mylogger')\\n *     logger.info('Logging')\\n *\\n *\\n * @param input the input DataFrame which has the same schema as the input schema to this stage.\\n * @param context a SparkExecutionPluginContext object that can be used to emit zero or more records (using the emitter.emit() method) or errors (using the emitter.emitError() method) \\n * @param context an object that provides access to:\\n *      1. CDAP Datasets and Streams - context.fromDataset('counts'); or context.fromStream('input');\\n *      2. Original Spark Context - context.getSparkContext();\\n *      3. Runtime Arguments - context.getArguments.get('priceThreshold')\\n */\\ndef sink(df: DataFrame, context: SparkExecutionPluginContext) : Unit = {\\n  df\\n}\"}\n      },\n      {\n        \"widget-type\": \"csv\",\n        \"name\": \"dependencies\",\n        \"label\": \"Dependencies\"\n      },\n      {\n        \"widget-type\": \"select\",\n        \"name\": \"deployCompile\",\n        \"label\": \"Compile at Deployment Time\",\n        \"widget-attributes\": {\n          \"default\": \"true\",\n          \"values\": [\n            \"true\",\n            \"false\"\n          ]\n        }\n      }\n    ]\n  }],\n  \"display-name\": \"Spark\",\n  \"icon\": {\n    \"arguments\": {\"data\": \"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAADY0lEQVRYR8XXd+i2YxQH8M+bTcab\\r\\nkZlVFJKtkJU/KHv1mplFyYgosikjWZFSRiIlkr0yMjJCsopCmSkrK1vfX9f963rv7uf3XM/zB+ev\\r\\n577vc53zvc74nvPM8z/LvCn8L4ubsT++wYm4fwo7M0emAXA7jqwc/oEN8fE0ICYFsAh+wDI9Z3fh\\r\\nsP8CwGZ4c4SjLeb4NhLbpBE4HHcUa99jKSxRnt/Fpvh7TCQ2wir4EJ9PCuAynFUcPIEXcWHl8Fjc\\r\\n0gOwBrZDwG+PFcv3z7DWpAAewF7FwFU4B+9hvfLuq+JkH+yKbbHyiIgkgvMnBfAR1i0Gj0I6Yhc8\\r\\nPUUBXoPTJgGwOH6rHG2JN8rzPThwBIif8RJ2q9o+79YOj7QCWAxn44LiJIWWAvy9PK+KRCfvIl/g\\r\\nQSRlTxXiqrnjFFw3johy4+Q7od4Ry1U3/KCQT33pY8qt4vT16sOhuLN6vg8HdM/9CCyKbQrTLcDy\\r\\nI8J67xwhr4+kXt7B0lVkNkBSMCMdgCVxRbltuH5IvsYjeAiP46cxhZfLvIrNq7TthBfqcx2AK3HG\\r\\ngMEU2cPF6Wv4Z4JqvxxnVvoX47z++Q7Ap1iz9/EmnI5fJnDaqabiQ1Sd/ZeR23dFO2uyU3i2KPR9\\r\\nJeyXIGAy9VolRZfi6+RuHDJ0uAOQnsxEC2UOySc4v8yBljRshaSsk7Tt+oidhaTfBSGT1MM6Ayhu\\r\\nQPr3r8YwPIOdK93bcPQ4APke0jmh3DiDIw6Px63lcEBnL/hzDJDd8WilE/1EOiQ1K3MxYdoxVZu2\\r\\ne646c2kBNwRgv6L7bdF/G5tUZ88tNdUEYOiCByGpyDzvJDScKRh5Cxvj+bInhl8ywjtJtyW9sztD\\r\\n6yyIga2L4fcrclm9EFhmfeTJMnTmys4eeKxTaAWQnH9ZZnuIaU+sgPR3mO24YrDffkNATsW1kwLI\\r\\nVpMNJpKV/OSS68yNbEih8UiW1SNwcNkThgBkUUmHzEhrBOLolXLmojKwUuWRvcvo7TsLF+yLbEcp\\r\\nxBBZSC3nZ6UVQP6EZAJGkorVyu9UeSj2u6GrVu8SwQyvrPQLSSuAk3B972xaM203zvmc2FoBpH/r\\r\\n0GXpSFRaWXEkiFYA+f93Y7GSEb0Dfh0T9qbPrQBWwtWFQJKOH5usNyi1AmgwNZ3Kv9VaoSGlep5g\\r\\nAAAAAElFTkSuQmCC\"},\n    \"type\": \"inline\"\n  }\n}",
    "doc.ScalaSparkCompute-sparkcompute": "# Spark Computation in Scala\n\nDescription\n-----------\nExecutes user-provided Spark code in Scala that transforms RDD to RDD with full\naccess to all Spark features.\n\nUse Case\n--------\nThis plugin can be used when you want to have complete control on the Spark computation.\nFor example, you may want to join the input RDD with another Dataset and select a subset\nof the join result using Spark SQL.\n\nProperties\n----------\n**scalaCode** Spark code in Scala defining how to transform RDD to RDD. \nThe code must implement a function called ``transform``, whose signature should be one of:\n\n    def transform(df: DataFrame) : DataFrame\n\n    def transform(df: DataFrame, context: SparkExecutionPluginContext) : DataFrame\n    \nThe input ``DataFrame`` has the same schema as the input schema to this stage and the ``transform`` method\nshould return a ``DataFrame`` that has the same schema as the output schema setup for this stage.\nUsing the ``SparkExecutionPluginContext``, you can access CDAP\nentities such as Stream and Dataset, as well as providing access to the underlying ``SparkContext`` in use.\n \nOperating on lower level ``RDD`` is also possible by using the one of the following forms of the ``transform`` method:\n\n    def transform(rdd: RDD[StructuredRecord]) : RDD[StructuredRecord]\n\n    def transform(rdd: RDD[StructuredRecord], context: SparkExecutionPluginContext) : RDD[StructuredRecord]\n   \nFor example:\n\n    def transform(rdd: RDD[StructuredRecord], context: SparkExecutionPluginContext) : RDD[StructuredRecord] = {\n      val outputSchema = context.getOutputSchema\n      rdd\n        .flatMap(_.get[String](\"body\").split(\"\\\\s+\"))\n        .map(s => (s, 1))\n        .reduceByKey(_ + _)\n        .map(t => StructuredRecord.builder(outputSchema).set(\"word\", t._1).set(\"count\", t._2).build)\n    }\n        \nThe will perform a word count on the input field ``'body'``, \nand produces records of two fields, ``'word'`` and ``'count'``.\n\nThe following imports are included automatically and are ready for the user code to use:\n\n      import io.cdap.cdap.api.data.format._\n      import io.cdap.cdap.api.data.schema._;\n      import io.cdap.cdap.etl.api.batch._\n      import org.apache.spark._\n      import org.apache.spark.api.java._\n      import org.apache.spark.rdd._\n      import org.apache.spark.sql._\n      import org.apache.spark.SparkContext._\n      import scala.collection.JavaConversions._\n\n\n**schema** The schema of output objects. If no schema is given, it is assumed that the output\nschema is the same as the input schema.\n\n**deployCompile** Specify whether the code will get validated during pipeline creation time. Setting this to `false`\nwill skip the validation.",
    "widgets.ScalaSparkProgram-sparkprogram": "{\n  \"outputs\": [],\n  \"metadata\": {\"spec-version\": \"1.4\"},\n  \"configuration-groups\": [{\n    \"label\": \"Spark Scala Program\",\n    \"properties\": [\n      {\n        \"widget-type\": \"textbox\",\n        \"name\": \"mainClass\",\n        \"label\": \"Main Class Name\",\n        \"widget-attributes\": {\"default\": \"SparkProgram\"}\n      },\n      {\n        \"widget-type\": \"scala-editor\",\n        \"name\": \"scalaCode\",\n        \"label\": \"Scala\",\n        \"widget-attributes\": {\"default\": \"import io.cdap.cdap.api.spark._\\nimport org.apache.spark._\\nimport org.slf4j._\\n\\nclass SparkProgram extends SparkMain {\\n  import SparkProgram._\\n\\n  override def run(implicit sec: SparkExecutionContext): Unit = {\\n    LOG.info(\\\"Spark Program Started\\\")\\n\\n    val sc = new SparkContext\\n\\n    LOG.info(\\\"Spark Program Completed\\\")\\n  }\\n}\\n\\nobject SparkProgram {\\n  val LOG = LoggerFactory.getLogger(getClass())\\n}\"}\n      },\n      {\n        \"widget-type\": \"csv\",\n        \"name\": \"dependencies\",\n        \"label\": \"Dependencies\"\n      },\n      {\n        \"widget-type\": \"select\",\n        \"name\": \"deployCompile\",\n        \"label\": \"Compile at Deployment Time\",\n        \"widget-attributes\": {\n          \"default\": \"true\",\n          \"values\": [\n            \"true\",\n            \"false\"\n          ]\n        }\n      }\n    ]\n  }],\n  \"icon\": {\n    \"arguments\": {\"data\": \"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAADY0lEQVRYR8XXd+i2YxQH8M+bTcab\\r\\nkZlVFJKtkJU/KHv1mplFyYgosikjWZFSRiIlkr0yMjJCsopCmSkrK1vfX9f963rv7uf3XM/zB+ev\\r\\n577vc53zvc74nvPM8z/LvCn8L4ubsT++wYm4fwo7M0emAXA7jqwc/oEN8fE0ICYFsAh+wDI9Z3fh\\r\\nsP8CwGZ4c4SjLeb4NhLbpBE4HHcUa99jKSxRnt/Fpvh7TCQ2wir4EJ9PCuAynFUcPIEXcWHl8Fjc\\r\\n0gOwBrZDwG+PFcv3z7DWpAAewF7FwFU4B+9hvfLuq+JkH+yKbbHyiIgkgvMnBfAR1i0Gj0I6Yhc8\\r\\nPUUBXoPTJgGwOH6rHG2JN8rzPThwBIif8RJ2q9o+79YOj7QCWAxn44LiJIWWAvy9PK+KRCfvIl/g\\r\\nQSRlTxXiqrnjFFw3johy4+Q7od4Ry1U3/KCQT33pY8qt4vT16sOhuLN6vg8HdM/9CCyKbQrTLcDy\\r\\nI8J67xwhr4+kXt7B0lVkNkBSMCMdgCVxRbltuH5IvsYjeAiP46cxhZfLvIrNq7TthBfqcx2AK3HG\\r\\ngMEU2cPF6Wv4Z4JqvxxnVvoX47z++Q7Ap1iz9/EmnI5fJnDaqabiQ1Sd/ZeR23dFO2uyU3i2KPR9\\r\\nJeyXIGAy9VolRZfi6+RuHDJ0uAOQnsxEC2UOySc4v8yBljRshaSsk7Tt+oidhaTfBSGT1MM6Ayhu\\r\\nQPr3r8YwPIOdK93bcPQ4APke0jmh3DiDIw6Px63lcEBnL/hzDJDd8WilE/1EOiQ1K3MxYdoxVZu2\\r\\ne646c2kBNwRgv6L7bdF/G5tUZ88tNdUEYOiCByGpyDzvJDScKRh5Cxvj+bInhl8ywjtJtyW9sztD\\r\\n6yyIga2L4fcrclm9EFhmfeTJMnTmys4eeKxTaAWQnH9ZZnuIaU+sgPR3mO24YrDffkNATsW1kwLI\\r\\nVpMNJpKV/OSS68yNbEih8UiW1SNwcNkThgBkUUmHzEhrBOLolXLmojKwUuWRvcvo7TsLF+yLbEcp\\r\\nxBBZSC3nZ6UVQP6EZAJGkorVyu9UeSj2u6GrVu8SwQyvrPQLSSuAk3B972xaM203zvmc2FoBpH/r\\r\\n0GXpSFRaWXEkiFYA+f93Y7GSEb0Dfh0T9qbPrQBWwtWFQJKOH5usNyi1AmgwNZ3Kv9VaoSGlep5g\\r\\nAAAAAElFTkSuQmCC\"},\n    \"type\": \"inline\"\n  }\n}",
    "doc.ScalaSparkSink-sparksink": "# Spark Sink in Scala\n\nDescription\n-----------\nExecutes user-provided Spark code in Scala that operates on an input RDD or Dataframe with full\naccess to all Spark features.\n\nUse Case\n--------\nThis plugin can be used when you want to have complete control on the Spark computation.\nFor example, you may want to join the input RDD with another Dataset and select a subset\nof the join result using Spark SQL before writing the results out to files in parquet format.\n\nProperties\n----------\n**scalaCode** Spark code in Scala defining how to transform RDD to RDD. \nThe code must implement a function called ``sink``, whose signature should be one of:\n\n    def sink(df: DataFrame) : DataFrame\n\n    def sink(df: DataFrame, context: SparkExecutionPluginContext) : DataFrame\n    \nThe input ``DataFrame`` has the same schema as the input schema to this stage.\nUsing the ``SparkExecutionPluginContext``, you can access CDAP\nentities such as Datasets, as well as providing access to the underlying ``SparkContext`` in use.\n \nOperating on lower level ``RDD`` is also possible by using the one of the following forms of the ``sink`` method:\n\n    def sink(rdd: RDD[StructuredRecord]) : RDD[StructuredRecord]\n\n    def sink(rdd: RDD[StructuredRecord], context: SparkExecutionPluginContext) : RDD[StructuredRecord]\n   \nFor example:\n\n    def sink(rdd: RDD[StructuredRecord], context: SparkExecutionPluginContext) : Unit = {\n      val outputSchema = context.getOutputSchema\n      rdd\n        .flatMap(_.get[String](\"body\").split(\"\\\\s+\"))\n        .map(s => (s, 1))\n        .reduceByKey(_ + _)\n        .saveAsTextFile(\"output\")\n    }\n        \nThis will perform a word count on the input field ``'body'``, then write out the results as a text file.\n\nThe following imports are included automatically and are ready for the user code to use:\n\n      import io.cdap.cdap.api.data.format._\n      import io.cdap.cdap.api.data.schema._;\n      import io.cdap.cdap.etl.api.batch._\n      import org.apache.spark._\n      import org.apache.spark.api.java._\n      import org.apache.spark.rdd._\n      import org.apache.spark.sql._\n      import org.apache.spark.SparkContext._\n      import scala.collection.JavaConversions._\n\n\n**deployCompile** Specify whether the code will get validated during pipeline creation time. Setting this to `false`\nwill skip the validation.",
    "widgets.PySparkProgram-sparkprogram": "{\n  \"outputs\": [],\n  \"metadata\": {\"spec-version\": \"1.4\"},\n  \"configuration-groups\": [{\n    \"label\": \"PySpark Program\",\n    \"properties\": [\n      {\n        \"widget-type\": \"python-editor\",\n        \"name\": \"pythonCode\",\n        \"label\": \"Python\",\n        \"widget-attributes\": {\"default\": \"from pyspark import *\\nfrom pyspark.sql import *\\nfrom cdap.pyspark import SparkExecutionContext\\n\\nsec = SparkExecutionContext()\\nsc = SparkContext()\"}\n      },\n      {\n        \"widget-type\": \"dsv\",\n        \"name\": \"pyFiles\",\n        \"label\": \"Extra python libraries\",\n        \"widget-attributes\": {\"delimiter\": \",\"}\n      }\n    ]\n  }],\n  \"display-name\": \"PySpark Program\",\n  \"icon\": {\n    \"arguments\": {\"data\": \"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADIAAAAyCAYAAAAeP4ixAAAABGdBTUEAALGPC/xhBQAAACBjSFJN\\r\\nAAB6JgAAgIQAAPoAAACA6AAAdTAAAOpgAAA6mAAAF3CculE8AAAACXBIWXMAAAsTAAALEwEAmpwY\\r\\nAAAEF2lUWHRYTUw6Y29tLmFkb2JlLnhtcAAAAAAAPHg6eG1wbWV0YSB4bWxuczp4PSJhZG9iZTpu\\r\\nczptZXRhLyIgeDp4bXB0az0iWE1QIENvcmUgNS40LjAiPgogICA8cmRmOlJERiB4bWxuczpyZGY9\\r\\nImh0dHA6Ly93d3cudzMub3JnLzE5OTkvMDIvMjItcmRmLXN5bnRheC1ucyMiPgogICAgICA8cmRm\\r\\nOkRlc2NyaXB0aW9uIHJkZjphYm91dD0iIgogICAgICAgICAgICB4bWxuczp4bXBNTT0iaHR0cDov\\r\\nL25zLmFkb2JlLmNvbS94YXAvMS4wL21tLyIKICAgICAgICAgICAgeG1sbnM6c3RSZWY9Imh0dHA6\\r\\nLy9ucy5hZG9iZS5jb20veGFwLzEuMC9zVHlwZS9SZXNvdXJjZVJlZiMiCiAgICAgICAgICAgIHht\\r\\nbG5zOnRpZmY9Imh0dHA6Ly9ucy5hZG9iZS5jb20vdGlmZi8xLjAvIgogICAgICAgICAgICB4bWxu\\r\\nczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iPgogICAgICAgICA8eG1wTU06RG9j\\r\\ndW1lbnRJRD54bXAuZGlkOjJEQjQxRDM5MTU0NjExRTZBNDQ1RjA5RjM3NDEyODhFPC94bXBNTTpE\\r\\nb2N1bWVudElEPgogICAgICAgICA8eG1wTU06RGVyaXZlZEZyb20gcmRmOnBhcnNlVHlwZT0iUmVz\\r\\nb3VyY2UiPgogICAgICAgICAgICA8c3RSZWY6aW5zdGFuY2VJRD54bXAuaWlkOjJEQjQxRDM2MTU0\\r\\nNjExRTZBNDQ1RjA5RjM3NDEyODhFPC9zdFJlZjppbnN0YW5jZUlEPgogICAgICAgICAgICA8c3RS\\r\\nZWY6ZG9jdW1lbnRJRD54bXAuZGlkOjJEQjQxRDM3MTU0NjExRTZBNDQ1RjA5RjM3NDEyODhFPC9z\\r\\ndFJlZjpkb2N1bWVudElEPgogICAgICAgICA8L3htcE1NOkRlcml2ZWRGcm9tPgogICAgICAgICA8\\r\\neG1wTU06SW5zdGFuY2VJRD54bXAuaWlkOjJEQjQxRDM4MTU0NjExRTZBNDQ1RjA5RjM3NDEyODhF\\r\\nPC94bXBNTTpJbnN0YW5jZUlEPgogICAgICAgICA8dGlmZjpPcmllbnRhdGlvbj4xPC90aWZmOk9y\\r\\naWVudGF0aW9uPgogICAgICAgICA8eG1wOkNyZWF0b3JUb29sPkFkb2JlIFBob3Rvc2hvcCBDQyAy\\r\\nMDE1IChNYWNpbnRvc2gpPC94bXA6Q3JlYXRvclRvb2w+CiAgICAgIDwvcmRmOkRlc2NyaXB0aW9u\\r\\nPgogICA8L3JkZjpSREY+CjwveDp4bXBtZXRhPgoy3jKuAAAOkUlEQVRoBe2ZeXDVRRLH33t5OUkC\\r\\ngUVIQiBE7kOh2HDIHSPnerGCsrsqhFO5NqCCRyHRpYBFjkIIGMICxbFLAMUFQSIQBAQUEShEObKc\\r\\ncUPAAIEEcr/9fCfv93wiWlrllvyRqZo3Pd09Pd09PT0zv2ezVZb/jwemTJnikOR58+bVnDVr1nuz\\r\\nZ89+UX0LL/hOxXkn5G+JO3bsmF3zl5SUTK9WrdpjN2/efGzOnDmnEhMTN2RkZDi7d+9e6nK57GvX\\r\\nrnXUrFnTfvnyZZf4zSABd0ORghQXq+GPIcd8fHzupW8rLS09Tdti/Pjxtyxjbtf3rlqRpKQkOdZV\\r\\nXFxcG8VjMOAUbX7VqlVb5+XlvQrtNa3IO++841tUVNQCejdwD+CAD+4qQ8LDw31QrFwrERISYr92\\r\\n7dp2p9O5gtX5BGVHz50792JZWVmLgoKC3r6+vnX9/PxsgYGBNvha3w2G2NnIMsA2YsSIUrWUGIyx\\r\\nsRpZ48aN28uGfz80NPRRcG87HA7blStXCli14xjXkFUB7fPqb2qIMhG1nGoMWLBgQTAeb8MKPEu1\\r\\nofQJWVVeXv5Kfn5+FIofoLuJ1TiMASswrsmNGzfeYO+s+cWb3T25kd+/f3/jSTJIGQg7fYcFd+vW\\r\\nzWfnzp1S0JpDrZR2KDM1b97cJSPS0tL8srKyekN7lPoHQqomre369et5NG0nTJhwUn3vQlpeUqNG\\r\\njYTc3Nw10J+CRkr4YTEKkdbspDeXWzHD5TaiXB0Z4UWTHJMGvXnMoJ/4Ia02xdvv+vv7N8HbNsIl\\r\\nh3DaS90MfitKXnAPt2OwY8CAAWWEWRIrMZl98SlGdyYcSzTn9wy5TTlLBcMDs2K5HE+3gFCItzO7\\r\\ndu3ageUPIyVuBl8NBZoDf9KzZ8/qhMaT0FYTv0GERX5AQEB49erV/3P16tVm4Bunp6evQ6lJ4KZ9\\r\\n++23RYzNBf8S58Uqa2KrxQgfGUFabg/fPgwugdZCq2WlY3OKaoCXEXaU6hQXF9efVNcGkjxtwkB8\\r\\nlBRqXwEomYjg1wXTPkfzhmAm+iseztm2bVse8ONkl+l4eJRWkPIcsJyhsoKNeyQoKMifuI8AvwLj\\r\\nzrFX/iyiPK2WcWpUvkLecfh9MfqBClTFr2G0jEDx5hhxEAV3w5iGcp/Tn081SQHjBoIvpYZ06NAh\\r\\nEBH5bLonevToEQXcnnq1d+/eNRkXBGxloGrAWdRrVBkczZgvBbNJv6lSpUosZ8IwcF9gsJ10m8ke\\r\\natOxY8eR7ghwWiE8duzY68z9N3iUACYvXbo0wDrpZYjZoJ07d5YC6cRra7x2DcE3NBl5ehSN8RD0\\r\\nIwjoA2051wfA8ld27dp1gdZOKA1iFUYHBwfnAc+C1yUH4cFlLP80lFyEHDs8U1HmkGQPHz7clxgv\\r\\nI5xS2dztGNN+/fr1ew4dOpRIKIaJhxAui42NbcUG7z5z5sz7FHpkqp0ckvUJ0xHiSUlJcSqMnMou\\r\\ntG8y+LXCwsJPMeRhVqUWE34A31oyxCu0ZZZnNJhVqEKcFwj+qaLwOHjwYMDGjRtvevNZUeCNA3bE\\r\\nxcVdxehQ9HgcvTYwz3MonDBw4MCjOHUwh+E2DFYodsZhZ6mtJk2alOdUZpIwiI3wrEC/3bt3X6a9\\r\\njJBYlL0kpEp8fHxVjByPVx9BQE2Mv8K4tfSnuZ1hnCJellz7aDj4+9SH9zhNEv39hJ8/TilC6UjG\\r\\nvkRtBu0wWWgZ3vZlFXWGHEXG/chPBs5ivvnAg4mEeHSwcZlUtEQzril1v0NpFkDlMAIloDWTvieE\\r\\njNCkgh988MFGCDhI6E3GY63gi6RtiWe0wTfJ8zJGvCgwgxXdBO8jtNHEdDS8vVBkH7SeW7ZsKSKU\\r\\nw2HdxfixGBAP7QGSWzC8gTg0m1of+sdSms09buLEicvBNcbQEazKRnS9zKqtoP+Z5pQRqi5tXib+\\r\\nlIlbisCgpcR2gmAMC6b5AnpDNmYG8N+ZOBthT9G+iDI+eGwc/PPgfQG+mfDdgi8Jgz9CmRrwTYUv\\r\\nFvj0jh077sWgRPhmo8wRaKvh3c6cjTB4NbKy6Zcxtg7tM/CvuD0UdQsYNWpUPnSVigPRYkIJDfwM\\r\\nweEYJGNmIGQSk05m0iQmxek7u5uh7h/GLGRvjUTxNDa6YlhKhDL2YXg3WbzIaI7hh1HOSbJohvyh\\r\\nhMZ4ZL7MHNPFR6hNgf46dBtG602yEhlPcwvwI5MVa9U5uxzMWYaO2hJaBBWXyVqC2rRp48sgpcke\\r\\nTJivJaW8yD6pT78bVf1Z+kFQgBVyTJwuHILthEY8CoTCu0NGwOdUdSvwNWyZ8NsInxDYozBWztKc\\r\\npjCuLnjJOocRJfAOwAHtZIRbTjn9UuiwukwkMdAoJkPKlY3ILCVSDgW+hHGQBMLswDtd4aktwxD8\\r\\nX82IJ123bt0y3oCnpZRDoXOMqSGY8o1+KE5i2NwIKrq2AMmBrxD+aDdsXUPEEqlIoKTAk0IU+CF/\\r\\nsdvJ2n/WCkiGMUDMKg4OnhCWdDqWrtEmrEDbjstblDJaeUYxLsMeE1J8GFxIAmgGboxCgfZd6hX3\\r\\nSnbp1KlTmHjkII0BboQMXQhzaYWL0jhkKxR1s5DSEVQpeYH2BcIuTwmFu5UVCbJSjpG3PEZpvM6R\\r\\nt4nx0cS4lJmP4L20ExFwP7gteOhN+ntlGBOUAc+B5yP6WomX4LsHvjVs9KeUiQit4+BDUXI//HOY\\r\\nQ578E/DHtG8x7hjtM9RD4HIZ32zr1q1XdCAz1xFqOM4w+wsHj0TWQrdzejHHVsZ5FxlTEVoIS8Xy\\r\\nEi0pSoymrpYR5PIcmJ5h4nCEyYgieHww+gVifCvtW3RlRBqn8tOSzvkj745kjI2waI+sNfCtB/dH\\r\\nqj8h6UerG24tria69uSTLc3VBd4oqjWXCTeSwCIc8iHjNP/7nGMxShrUPl26dNH5ISPMyvicPXs2\\r\\nJzo6OoPJ9XBhXOkZPPBPhA5GkGj9UCqOvZfKoGToupvk0O6hPxkvvZmdna04NMt95syZozExMenQ\\r\\ntR9uSh60t6gHgHU/W4UDzuOoqsBpy5YtM+cAY0LgD0D2DugbT58+bcK8QYMGuu36YUgmtDz0+pq2\\r\\nLvXG+fPn5eyKEHPHG32bzcpEpuP+IfTm9+rVy0U70hvvBUuQ2eHCKZV70X4uWKHMD7k9cn9IMpjv\\r\\nj3NP7kGitEmbYgXe17dvXxfL2U99ZRC1GiM+QM844VVE8zJIDzUZ5xC/tVEFe/FANvvVM68Q7mLG\\r\\nec11581ucbtbSylPasOA4dCiCbtkMo9yvrwkuocH+MeK5P0cvh8bX4mv9EClB34lD5jNzUYmTdtd\\r\\nM2bMCCFnP08+j6Dq9lnAgbeBzzbK9b/Wxv2enKFDhyqZVE1NTZ2pLOb9Cv0lNpo8zcdjk1JRfgan\\r\\neCTts5zIYzl8Xubg2c9nS73yXPqYdgfh5rksJbzSq0nB3meU6O5060pISPg9NcYtqzVtA8F16tTx\\r\\nc/MYksZb434MD485AuQdj4cwaCVGFLMSbTmFm1B1dQng5O63atUq82q0Vs/M5DXW3TeNJvXyrO7c\\r\\numJ40jCrcADGiazCDuB50EuWLFkywUuGzgrvW7MhyTDqHVO/jLBNnTq1JwbciwGNWYFmhNdFjOgM\\r\\nqR73KFu7du0m8IHsOo+mryIjI/d6C8SzIaziX+DVfxnFGD530aJFl1Awjn7h4sWL92qOYcOG9abJ\\r\\nhreYuT4G/pA5hrHqkzFE37SWQ+tL/YCvIhkaM2jQID2lB0P3oa7CWHOdQXZT+g/DUo85vsQhC53T\\r\\npk17nrvUApTXHyqq5RAdMOo27EJ5O8q3hedJ3hblFy5caBwVFZUJyQGfi4k3AB/AgHkoM4HxehW2\\r\\npeoj3WFaYwjwCOBjtFNoC6lnuGcVolQx8EPI2g7tKDLe5zNRR+TeQNY6+hPFD30dzngE2Be+VPqj\\r\\nqRnAG4cMGRLggHEQjyQbSuqCZ+Mm7ODrn/5zEM4OnMWXlpv0y9g34u+DMBXXmDFj/BEUhYcX4cXz\\r\\neGYc+PoI1pfELCbSu90U+K4ClMCnt8hF+ttEoK2BzHV4ewV1Of09yItFl0TgdHAbqB8CL6VOompv\\r\\nXWKu3az2Afr/oH+fk3AKoyOBQSie2bJlyz36hIlw/XNk41ZaEBERoc8/5jJIW1cKqJDRtHqFXPtr\\r\\n0T0rHOUKtT5V//VpI5oCnydRAPtSa4hAa92cLT6tlkI+glXZZ5D8AJ9EXieBVM9+A/ZHRqE+BOTz\\r\\nvtCfJxdZ5nQUH4BHAmAoxZhyGQqPHbCA8KmCsOvQTOHDmZ0vfqJdsnC0eu7K48HU8154ffGQEnKa\\r\\nYj7XDUsp84FAfZSSEXoK67ncUDgV+OtRL1Glh8cQ2DXWJUMyUKYVChfUrl27Ed79HQq6aCXQFDac\\r\\nJqiiFcLIzULS19W+nGeoPDIcJywArcxzSUtOeOlb1Uja7UysZ2x3aEupKnKGQnQXRgdB86wccCjj\\r\\narEnp7Pi+9gXXTFKRj8N7xPQW1Or07dKCECINnVSTk5OMvtkDYhCBst6O3vlM77mbaa/jXYlYbcG\\r\\nQ0Y0adLkc+jGs+wdbXjtAz2BFdOB8DykGYjh6dD0lVB4nRVnaI0nUSwBWiCbui6o1cDvaoy7pED/\\r\\nPDk5+SL4gcCP0w5BzvM4SM/kr6lvW8zQlGyk+3eFF9e/McrFK68wMzMzh/reiRMndp88eTL+Oy4D\\r\\nmdXivPBjJY7g9XredKVn775g+Fbi3WG3460+ykimJwqQ4Vkli+c2nPUuMWSnBPBVxK9Pnz5FwNl6\\r\\nH/N9yp8/YO7hc6re03l4xXwZP3XqlH/Dhg2tLy22sLAwFzQ9WbXZz8kwPqaVMqE2sI3W79y5cy0I\\r\\nrX7I1lnxL+F1YOpTLe8b8Zq/4vCs2Seiuf+WKxUsfqvA6+HXgUtf/9tUjBeTjKFoJaqxB15jwjhq\\r\\nIPiLkBc3bdp0tcVjCbVaPD2QZLGLQ1CbUx71bERdWXhz6yP1LeQu1LmhianGUEtGZVvpgUoPVHrg\\r\\nZ3vgfwqL3zBFmCLvAAAAAElFTkSuQmCC\"},\n    \"type\": \"inline\"\n  }\n}"
  },
  "parents": [
    "system:cdap-data-pipeline[6.0.0-SNAPSHOT,7.0.0-SNAPSHOT)",
    "system:cdap-data-streams[6.0.0-SNAPSHOT,7.0.0-SNAPSHOT)"
  ]
}